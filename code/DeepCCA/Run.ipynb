{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "Run.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYl9QPeq9dXA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from linear_cca import linear_cca\n",
        "from torch.utils.data import BatchSampler, SequentialSampler\n",
        "from DeepCCAModels import DeepCCA\n",
        "from main import Solver\n",
        "from utils import load_data, svm_classify\n",
        "try:\n",
        "    import cPickle as thepickle\n",
        "except ImportError:\n",
        "    import _pickle as thepickle\n",
        "\n",
        "import gzip\n",
        "import numpy as np\n",
        "torch.set_default_tensor_type(torch.DoubleTensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6B_7VVfz9dXH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3687e2ec-7ff4-42f4-91b0-19f93ffb7cc2"
      },
      "source": [
        "############\n",
        "# Parameters Section\n",
        "\n",
        "device = torch.device('cuda')\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPUs\")\n",
        "\n",
        "# the path to save the final learned features\n",
        "save_to = './new_features.gz'\n",
        "\n",
        "# the size of the new space learned by the model (number of the new features)\n",
        "outdim_size = 15\n",
        "\n",
        "# size of the input for view 1 and view 2\n",
        "input_shape1 = 784\n",
        "input_shape2 = 784\n",
        "\n",
        "# number of layers with nodes in each one\n",
        "layer_sizes1 = [1024, 1024, 1024, outdim_size]\n",
        "layer_sizes2 = [1024, 1024, 1024, outdim_size]\n",
        "\n",
        "# the parameters for training the network\n",
        "learning_rate = 1e-3\n",
        "epoch_num = 15\n",
        "batch_size = 800\n",
        "\n",
        "# the regularization parameter of the network\n",
        "# seems necessary to avoid the gradient exploding especially when non-saturating activations are used\n",
        "reg_par = 1e-5\n",
        "\n",
        "# specifies if all the singular values should get used to calculate the correlation or just the top outdim_size ones\n",
        "# if one option does not work for a network or dataset, try the other one\n",
        "use_all_singular_values = False\n",
        "\n",
        "# if a linear CCA should get applied on the learned features extracted from the networks\n",
        "# it does not affect the performance on noisy MNIST significantly\n",
        "apply_linear_cca = True\n",
        "# end of parameters section\n",
        "############\n",
        "\n",
        "# Each view is stored in a gzip file separately. They will get downloaded the first time the code gets executed.\n",
        "# Datasets get stored under the datasets folder of user's Keras folder\n",
        "# normally under [Home Folder]/.keras/datasets/\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 1 GPUs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDjb1k5G9dXK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c65d5d50-3f6f-41bc-b291-3153a5db9063"
      },
      "source": [
        "data1 = load_data('./noisymnist_view1.gz')\n",
        "data2 = load_data('./noisymnist_view2.gz')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading data ...\n",
            "loading data ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "bbuQjDRi9dXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2b7a0ae6-9aae-45c9-9664-e72fd66aae7a"
      },
      "source": [
        "# Building, training, and producing the new features by DCCA\n",
        "model = DeepCCA(layer_sizes1, layer_sizes2, input_shape1,\n",
        "                input_shape2, outdim_size, use_all_singular_values, device=device).double()\n",
        "l_cca = None\n",
        "if apply_linear_cca:\n",
        "    l_cca = linear_cca()\n",
        "solver = Solver(model, l_cca, outdim_size, epoch_num, batch_size,\n",
        "                learning_rate, reg_par, device=device)\n",
        "train1, train2 = data1[0][0], data2[0][0]\n",
        "val1, val2 = data1[1][0], data2[1][0]\n",
        "test1, test2 = data1[2][0], data2[2][0]\n",
        "# val1=None\n",
        "# test1=None\n",
        "solver.fit(train1, train2, val1, val2, test1, test2)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ INFO : 2020-05-12 13:35:37,873 ] - DataParallel(\n",
            "  (module): DeepCCA(\n",
            "    (model1): MlpNet(\n",
            "      (layers): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (3): Linear(in_features=1024, out_features=15, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (model2): MlpNet(\n",
            "      (layers): ModuleList(\n",
            "        (0): Sequential(\n",
            "          (0): Linear(in_features=784, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (1): Sequential(\n",
            "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (2): Sequential(\n",
            "          (0): Linear(in_features=1024, out_features=1024, bias=True)\n",
            "          (1): Sigmoid()\n",
            "        )\n",
            "        (3): Linear(in_features=1024, out_features=15, bias=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "[ INFO : 2020-05-12 13:35:37,875 ] - RMSprop (\n",
            "Parameter Group 0\n",
            "    alpha: 0.99\n",
            "    centered: False\n",
            "    eps: 1e-08\n",
            "    lr: 0.001\n",
            "    momentum: 0\n",
            "    weight_decay: 1e-05\n",
            ")\n",
            "[ INFO : 2020-05-12 13:35:46,680 ] - Epoch 1: val_loss improved from 0.0000 to -6.1933, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:35:46,738 ] - Epoch 1/15 - time: 8.66 - training_loss: -5.1357 - val_loss: -6.1933\n",
            "[ INFO : 2020-05-12 13:35:55,283 ] - Epoch 2: val_loss improved from -6.1933 to -7.0933, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:35:55,340 ] - Epoch 2/15 - time: 8.60 - training_loss: -5.8821 - val_loss: -7.0933\n",
            "[ INFO : 2020-05-12 13:36:03,873 ] - Epoch 3: val_loss improved from -7.0933 to -7.5047, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:36:03,931 ] - Epoch 3/15 - time: 8.59 - training_loss: -6.3712 - val_loss: -7.5047\n",
            "[ INFO : 2020-05-12 13:36:12,391 ] - Epoch 4: val_loss improved from -7.5047 to -7.7072, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:36:12,445 ] - Epoch 4/15 - time: 8.51 - training_loss: -6.7240 - val_loss: -7.7072\n",
            "[ INFO : 2020-05-12 13:36:20,895 ] - Epoch 5: val_loss improved from -7.7072 to -7.8166, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:36:20,955 ] - Epoch 5/15 - time: 8.51 - training_loss: -6.9944 - val_loss: -7.8166\n",
            "[ INFO : 2020-05-12 13:36:29,479 ] - Epoch 6: val_loss improved from -7.8166 to -8.1588, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:36:29,541 ] - Epoch 6/15 - time: 8.59 - training_loss: -7.1929 - val_loss: -8.1588\n",
            "[ INFO : 2020-05-12 13:36:38,056 ] - Epoch 7: val_loss did not improve from -8.1588\n",
            "[ INFO : 2020-05-12 13:36:38,057 ] - Epoch 7/15 - time: 8.52 - training_loss: -7.3568 - val_loss: -7.8764\n",
            "[ INFO : 2020-05-12 13:36:46,569 ] - Epoch 8: val_loss did not improve from -8.1588\n",
            "[ INFO : 2020-05-12 13:36:46,570 ] - Epoch 8/15 - time: 8.51 - training_loss: -7.4911 - val_loss: -8.0787\n",
            "[ INFO : 2020-05-12 13:36:55,125 ] - Epoch 9: val_loss improved from -8.1588 to -8.2058, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:36:55,182 ] - Epoch 9/15 - time: 8.61 - training_loss: -7.6065 - val_loss: -8.2058\n",
            "[ INFO : 2020-05-12 13:37:03,817 ] - Epoch 10: val_loss improved from -8.2058 to -8.2969, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:37:03,877 ] - Epoch 10/15 - time: 8.69 - training_loss: -7.7111 - val_loss: -8.2969\n",
            "[ INFO : 2020-05-12 13:37:12,551 ] - Epoch 11: val_loss did not improve from -8.2969\n",
            "[ INFO : 2020-05-12 13:37:12,552 ] - Epoch 11/15 - time: 8.67 - training_loss: -7.8053 - val_loss: -8.1867\n",
            "[ INFO : 2020-05-12 13:37:21,253 ] - Epoch 12: val_loss improved from -8.2969 to -8.5835, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:37:21,307 ] - Epoch 12/15 - time: 8.75 - training_loss: -7.8990 - val_loss: -8.5835\n",
            "[ INFO : 2020-05-12 13:37:29,992 ] - Epoch 13: val_loss improved from -8.5835 to -8.5856, saving model to checkpoint.model\n",
            "[ INFO : 2020-05-12 13:37:30,051 ] - Epoch 13/15 - time: 8.74 - training_loss: -7.9830 - val_loss: -8.5856\n",
            "[ INFO : 2020-05-12 13:37:38,724 ] - Epoch 14: val_loss did not improve from -8.5856\n",
            "[ INFO : 2020-05-12 13:37:38,725 ] - Epoch 14/15 - time: 8.67 - training_loss: -8.0683 - val_loss: -8.4063\n",
            "[ INFO : 2020-05-12 13:37:47,385 ] - Epoch 15: val_loss did not improve from -8.5856\n",
            "[ INFO : 2020-05-12 13:37:47,385 ] - Epoch 15/15 - time: 8.66 - training_loss: -8.1514 - val_loss: -8.2215\n",
            "[ INFO : 2020-05-12 13:37:51,372 ] - loss on validation data: -8.5856\n",
            "[ INFO : 2020-05-12 13:37:52,032 ] - loss on test data: -8.6557\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KeD_hD1e9dXc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "a71f04fe-65ef-4bbf-ef2a-d02d1fb69b59"
      },
      "source": [
        "set_size = [0, train1.size(0), train1.size(\n",
        "    0) + val1.size(0), train1.size(0) + val1.size(0) + test1.size(0)]\n",
        "loss, outputs = solver.test(torch.cat([train1, val1, test1], dim=0), torch.cat(\n",
        "    [train2, val2, test2], dim=0), apply_linear_cca)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Linear CCA started!\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KgJrysPP9dXf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "l_cca.fit(np.array(train1), np.array(train2), 15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxdGXm7i9dXi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "f022bfa3-72b5-4e3c-985e-388a6fc968b3"
      },
      "source": [
        "v1, v2 = l_cca.test(np.array(torch.cat([train1, val1, test1], dim=0)), np.array(torch.cat([train2, val2, test2], dim=0)))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mijnz-jw9dXl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "b94afe46-e20f-4b58-840c-dbe66cdb4957"
      },
      "source": [
        "new_data = []\n",
        "for idx in range(3):\n",
        "    new_data.append([outputs[0][set_size[idx]:set_size[idx + 1], :], data1[idx][1]])\n",
        "# Training and testing of SVM with linear kernel on the view 1 with new features\n",
        "[test_acc, valid_acc] = svm_classify(new_data, C=0.01)\n",
        "print(\"Accuracy on view 1 (validation data) is:\", valid_acc * 100.0)\n",
        "print(\"Accuracy on view 1 (test data) is:\", test_acc*100.0)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training SVM...\n",
            "Accuracy on view 1 (validation data) is: 93.37\n",
            "Accuracy on view 1 (test data) is: 92.91\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoL8FFv59dXo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "d60b41ea-5123-4d2c-b5ce-883a637d1ad4"
      },
      "source": [
        "new_data = []\n",
        "for idx in range(3):\n",
        "    new_data.append([v1[set_size[idx]:set_size[idx + 1], :], data1[idx][1]])\n",
        "# Training and testing of SVM with linear kernel on the view 1 with new features\n",
        "[test_acc, valid_acc] = svm_classify(new_data, C=0.01)\n",
        "print(\"Accuracy on view 1 (validation data) is:\", valid_acc * 100.0)\n",
        "print(\"Accuracy on view 1 (test data) is:\", test_acc*100.0)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training SVM...\n",
            "Accuracy on view 1 (validation data) is: 75.96000000000001\n",
            "Accuracy on view 1 (test data) is: 75.96000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8ISsD9C9dX4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=15)\n",
        "pca.fit(train1)\n",
        "\n",
        "t1 = pca.transform(test1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXHMMEnMD3js",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "A = torch.from_numpy(pca.transform(train1))\n",
        "B = torch.from_numpy(pca.transform(val1))\n",
        "C = torch.from_numpy(pca.transform(test1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9G7vPMaFkxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "set_size = [0, train1.size(0), train1.size(\n",
        "    0) + val1.size(0), train1.size(0) + val1.size(0) + test1.size(0)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p64GNq29dX8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0f12d19a-a26a-4f0e-8646-787cda7bb34d"
      },
      "source": [
        "new_data = []\n",
        "for idx in range(3):\n",
        "    new_data.append([np.array(torch.cat([A, B, C], dim=0))[set_size[idx]:set_size[idx + 1], :], data1[idx][1]])\n",
        "# Training and testing of SVM with linear kernel on the view 1 with new features\n",
        "[test_acc, valid_acc] = svm_classify(new_data, C=0.01)\n",
        "print(\"Accuracy on view 1 (validation data) is:\", valid_acc * 100.0)\n",
        "print(\"Accuracy on view 1 (test data) is:\", test_acc*100.0)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training SVM...\n",
            "Accuracy on view 1 (validation data) is: 72.84\n",
            "Accuracy on view 1 (test data) is: 72.25\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}